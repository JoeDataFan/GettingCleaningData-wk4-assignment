---
title: "README"
author: "Joe Rubash"
date: "September 11, 2016"
output: html_document
---

README file for Getting and Cleaning Data week 4 final assignment

Summary:
========
This README document describes the steps taken to download, read, merge, tidy and summarize data sets from the Human Activity Recognition Using Smartphones Data Sets. The goal of this exercise was to demonstrate my ability to collect, work with, and clean a data set. The ultimate goal was to prepare two tidy data sets that could then be used for later analysis. An R script (refer to file "run_analysis.R") was written to accomplish this task. The R script downloads the required zip folder, unzips the contents of the folder, extracts test and train data sets as well as subject, activity and variable label files, then extracts only columns with mean and standard deviation data, then combines all test and train data with appropriate labels the resulting tidy data set is labeled "tt_comb". The R script then creates a second tidy data set which is the average of all variables in "tt_comb" grouped by subject and by activity; this 2nd tidy data set is labeled "tt_comb_sum". The R script will aslo write a csv file titled "summary_activity_data.csv" to the working directory. Further details about how the orignial data sets were obtained and organized is included below.

Note on final format of "tidy" data set:
=======================================
It should also be noted there are naturally several variables combined within each column. These variable could be pulled apart to create other columns such as "axis" and "type of motion" among others. However, it was decided not to alter these variables becuase it was outside of the scope of the assignment and the decision to do so should be based on the specific need and how you define "tidy".

Reference:
=========
- The associated R script "run_analysis.R" includes more details on how tidy data sets were generated. 
- The complete list of variables for both tidy sets "tt_comb" and "tt_comb_sum" are listed in the associated "Codebook.RMD" file.

Raw data:
========
The following data sets were combined to create the tidy data sets:
- 'features.txt': List of all features or variable names.
- 'activity_labels.txt': Links the class labels with their activity name.
- 'train/X_train.txt': Training set.
- 'train/y_train.txt': Training labels.
- 'test/X_test.txt': Test set.
- 'test/y_test.txt': Test labels.
For more information about this dataset contact: activityrecognition@smartlab.ws

Note on variable naming:
=======================
The original feature (variable) names were used to label columns becuase they conveyed the most information the most clearly. 

variable names (other than subject and activity) have 4 parts; for example "tBodyAcc-mean()-X" can be broken in to "t", "BodyAcc", "mean()" and "X". Each part describes details of the measurement. Listed below are all measurement options by part and their definitions:

Part 1.
"t" = time domain signal
"f" = frequency domain signals

Part 2.
"BodyAcc" = body linear acceleration
"GravityAcc" = gravity linear acceleration
"BodyAccJerk" = jerking motion linear acceleration
"BodyGyro" = body angluar velocity
"BodyGyroJerk" = jeking motion angular velocity 
"BodyAccMag" = magnitude of body linear acceleration
"GravityAccMag" = magnitude of gravity linear acceleration
"BodyAccJerkMag" = magnitude of jerking motion linear acceleration
"BodyGyroMag" = magnitude of body angular velocity
"BodyGyroJerkMag" = magnitude of jerking motion angular velocity

Part 3.
"mean()" = mean of specific measurement
"std()" = standard deviation of specific measurement
"meanFreq" = weighted average of the frequency components to obtain a mean frequency

Part 4.
"X" = measurement taken along X axis
"Y" = measurement taken along Y axis
"Z" = measurement taken along Z axis



Study details 
=====================================================
Human Activity Recognition Using Smartphones Data Set 

Abstract: Human Activity Recognition database built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors.

Data Set Characteristics:  
Multivariate, Time-Series
Number of Instances:10299
Area:Computer
Attribute Characteristics:N/A
Number of Attributes:561
Date Donated 2012-12-10
Associated Tasks:
Classification, Clustering
Missing Values? N/A
Number of Web Hits: 316780

Source:
Jorge L. Reyes-Ortiz(1,2), Davide Anguita(1), Alessandro Ghio(1), Luca Oneto(1) and Xavier Parra(2)
1 - Smartlab - Non-Linear Complex Systems Laboratory
DITEN - Università degli Studi di Genova, Genoa (I-16145), Italy. 
2 - CETpD - Technical Research Centre for Dependency Care and Autonomous Living
Universitat Politècnica de Catalunya (BarcelonaTech). Vilanova i la Geltrú (08800), Spain
activityrecognition '@' smartlab.ws


Original Data Set Information:

The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. 

The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.

A video of the experiment including an example of the 6 recorded activities with one of the participants can be seen in the following link: https://www.youtube.com/watch?v=XOEN9W05_4A

An updated version of this dataset can be found at http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions. It includes labels of postural transitions between activities and also the full raw inertial signals instead of the ones pre-processed into windows.

Attribute Information:

For each record in the dataset it is provided: 
- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration. 
- Triaxial Angular velocity from the gyroscope. 
- A 561-feature vector with time and frequency domain variables. 
- Its activity label. 
- An identifier of the subject who carried out the experiment.

The features selected for this database come from the accelerometer and gyroscope 3-axial raw signals tAcc-XYZ and tGyro-XYZ. These time domain signals (prefix 't' to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. Similarly, the acceleration signal was then separated into body and gravity acceleration signals (tBodyAcc-XYZ and tGravityAcc-XYZ) using another low pass Butterworth filter with a corner frequency of 0.3 Hz. 

Subsequently, the body linear acceleration and angular velocity were derived in time to obtain Jerk signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ). Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm (tBodyAccMag, tGravityAccMag, tBodyAccJerkMag, tBodyGyroMag, tBodyGyroJerkMag). 

Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. (Note the 'f' to indicate frequency domain signals). 


These signals were used to estimate variables of the feature vector for each pattern:  
'-XYZ' is used to denote 3-axial signals in the X, Y and Z directions.

tBodyAcc-XYZ
tGravityAcc-XYZ
tBodyAccJerk-XYZ
tBodyGyro-XYZ
tBodyGyroJerk-XYZ
tBodyAccMag
tGravityAccMag
tBodyAccJerkMag
tBodyGyroMag
tBodyGyroJerkMag
fBodyAcc-XYZ
fBodyAccJerk-XYZ
fBodyGyro-XYZ
fBodyAccMag
fBodyAccJerkMag
fBodyGyroMag
fBodyGyroJerkMag

The set of variables that were estimated from these signals are: 

mean(): Mean value
std(): Standard deviation
meanFreq(): Weighted average of the frequency components to obtain a mean frequency



Relevant Papers:

Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012 

Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L. Reyes-Ortiz. Energy Efficient Smartphone-Based Activity Recognition using Fixed-Point Arithmetic. Journal of Universal Computer Science. Special Issue in Ambient Assisted Living: Home Care. Volume 19, Issue 9. May 2013

Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. 4th International Workshop of Ambient Assited Living, IWAAL 2012, Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings. Lecture Notes in Computer Science 2012, pp 216-223. 

Jorge Luis Reyes-Ortiz, Alessandro Ghio, Xavier Parra-Llanas, Davide Anguita, Joan Cabestany, Andreu Català. Human Activity and Motion Disorder Recognition: Towards Smarter Interactive Cognitive Environments. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.



Citation Request:

Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. 


License:
========
Use of this dataset in publications must be acknowledged by referencing the following publication [1] 

[1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012

This dataset is distributed AS-IS and no responsibility implied or explicit can be addressed to the authors or their institutions for its use or misuse. Any commercial use is prohibited.

Jorge L. Reyes-Ortiz, Alessandro Ghio, Luca Oneto, Davide Anguita. November 2012.